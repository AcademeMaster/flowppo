import argparse
import gymnasium as gym
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import minari
from collections import deque
import random
import tqdm
import os
import time
import math


# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯å¤ç°
def set_seed(seed=42):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


# ================== æ ¸å¿ƒç½‘ç»œæ¶æ„ ==================
class FourierFeatures(nn.Module):
    """å‚…é‡Œå¶ç‰¹å¾ç¼–ç ï¼ˆé«˜æ•ˆæ—¶é—´åµŒå…¥ï¼‰"""

    def __init__(self, input_dim, output_dim, scale=10.0):
        super().__init__()
        self.B = nn.Parameter(torch.randn(input_dim, output_dim // 2) * scale, requires_grad=False)

    def forward(self, t):
        t_proj = 2 * np.pi * t @ self.B
        return torch.cat([torch.sin(t_proj), torch.cos(t_proj)], dim=-1)


class VectorizedFlowField(nn.Module):
    """æ”¯æŒæ‰¹é‡å¤šæ ·æœ¬ç”Ÿæˆçš„æµåœºç½‘ç»œ"""

    def __init__(self, obs_dim, action_dim, horizon_length,
                 fourier_dim=64, hidden_dims=[256, 256]):
        super().__init__()
        self.time_encoder = FourierFeatures(1, fourier_dim)
        self.input_dim = obs_dim + action_dim * horizon_length + fourier_dim
        self.output_dim = action_dim * horizon_length

        # ä¿®å¤1: æ›´ç¨³å®šçš„ç½‘ç»œåˆå§‹åŒ–ï¼Œé¿å…BatchNormåœ¨è¯„ä¼°æ—¶çš„é—®é¢˜
        layers = []
        prev_dim = self.input_dim
        for hidden_dim in hidden_dims:
            linear = nn.Linear(prev_dim, hidden_dim)
            # Xavieråˆå§‹åŒ–ï¼Œæ›´ç¨³å®š
            nn.init.xavier_uniform_(linear.weight)
            nn.init.zeros_(linear.bias)
            layers.append(linear)
            layers.append(nn.ReLU())
            # ä¿®å¤ï¼šä½¿ç”¨LayerNormæ›¿ä»£BatchNormï¼Œé¿å…è¯„ä¼°æ—¶çš„ç»´åº¦é—®é¢˜
            layers.append(nn.LayerNorm(hidden_dim))
            prev_dim = hidden_dim

        # è¾“å‡ºå±‚ä½¿ç”¨å°çš„åˆå§‹åŒ–
        output_layer = nn.Linear(prev_dim, self.output_dim)
        nn.init.xavier_uniform_(output_layer.weight, gain=0.01)  # å°å¢ç›Š
        nn.init.zeros_(output_layer.bias)
        layers.append(output_layer)

        self.net = nn.Sequential(*layers)

    def forward(self, obs, actions, t):
        t_feat = self.time_encoder(t)
        # å‘é‡åŒ–å¤„ç†ï¼šæ”¯æŒä»»æ„æ‰¹æ¬¡ç»´åº¦
        x = torch.cat([obs, actions, t_feat], dim=-1)
        return self.net(x)


class ParallelCritic(nn.Module):
    """æ”¯æŒæ‰¹é‡å¤šæ ·æœ¬è¯„ä¼°çš„Criticç½‘ç»œ"""

    def __init__(self, obs_dim, action_dim, horizon_length,
                 hidden_dims=[256, 256]):
        super().__init__()
        self.horizon_length = horizon_length
        self.action_dim = action_dim
        input_dim = obs_dim + action_dim * horizon_length

        # ä¿®å¤2: æ›´ç¨³å®šçš„Criticç½‘ç»œè®¾è®¡
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            linear = nn.Linear(prev_dim, hidden_dim)
            nn.init.xavier_uniform_(linear.weight)
            nn.init.zeros_(linear.bias)
            layers.append(linear)
            layers.append(nn.ReLU())
            layers.append(nn.LayerNorm(hidden_dim))  # LayerNormæ›´ç¨³å®š
            prev_dim = hidden_dim

        # è¾“å‡ºå±‚ç‰¹æ®Šåˆå§‹åŒ–ï¼Œé¿å…Qå€¼çˆ†ç‚¸
        output_layer = nn.Linear(prev_dim, 1)
        nn.init.xavier_uniform_(output_layer.weight, gain=0.1)  # æ›´å°çš„å¢ç›Š
        nn.init.zeros_(output_layer.bias)
        layers.append(output_layer)

        self.net = nn.Sequential(*layers)

    def forward(self, obs, actions):
        # ç¡®ä¿åŠ¨ä½œå—çš„ç»´åº¦æ­£ç¡®
        batch_size = obs.shape[0]
        if actions.dim() > 2:
            actions = actions.view(batch_size, -1)  # å±•å¹³åŠ¨ä½œå—
        elif actions.dim() == 2 and actions.shape[1] != self.horizon_length * self.action_dim:
            actions = actions.view(batch_size, -1)  # å±•å¹³åŠ¨ä½œå—
            
        # åˆå¹¶çŠ¶æ€å’ŒåŠ¨ä½œ
        x = torch.cat([obs, actions], dim=-1)
        result = self.net(x).squeeze(-1)  # [batch_size]
        return result


# ================== é«˜æ•ˆå›æ”¾ï¿½ï¿½ï¿½å†²åŒº ==================
class VectorizedReplayBuffer:
    """æ”¯æŒæ‰¹é‡å­˜å‚¨å’Œæ£€ç´¢çš„ä¼˜åŒ–ç¼“å†²åŒº"""

    def __init__(self, horizon_length=5, capacity=1000000):
        self.buffer = deque(maxlen=capacity)
        self.horizon_length = horizon_length
        self._prealloc_buffers = {}

    def _preallocate_buffers(self, sample_shape):
        """é¢„åˆ†é…å†…å­˜åŠ é€Ÿæ‰¹é‡æ“ä½œ"""
        for key, shape in sample_shape.items():
            self._prealloc_buffers[key] = np.empty(
                (self.buffer.maxlen, *shape),
                dtype=np.float32
            )
        self._index = 0
        self._full = False

    def add_trajectory(self, observations, actions, rewards, terminations):
        """æ·»åŠ Minariæ ¼å¼çš„è½¨è¿¹"""
        T = len(observations)
        for i in range(T - self.horizon_length):
            # æå–è½¨è¿¹å—
            obs = observations[i]
            next_obs = observations[i + self.horizon_length]
            chunk_actions = actions[i:i + self.horizon_length].flatten()
            chunk_rewards = rewards[i:i + self.horizon_length]
            done = any(terminations[i:i + self.horizon_length])

            # å¡«å……ä¸è¶³éƒ¨åˆ†
            if len(chunk_rewards) < self.horizon_length:
                chunk_rewards = np.pad(
                    chunk_rewards,
                    (0, self.horizon_length - len(chunk_rewards)),
                    'constant'
                )

            # å­˜å‚¨åˆ°ç¼“å†²åŒº
            item = (obs, chunk_actions, chunk_rewards, next_obs, done)

            if self._prealloc_buffers:
                if self._index >= self.buffer.maxlen:
                    self._full = True
                    self._index = 0

                self._prealloc_buffers['observations'][self._index] = obs
                self._prealloc_buffers['actions_chunk'][self._index] = chunk_actions
                self._prealloc_buffers['rewards_chunk'][self._index] = chunk_rewards
                self._prealloc_buffers['next_observations'][self._index] = next_obs
                self._prealloc_buffers['terminations'][self._index] = done
                self._index += 1
            else:
                self.buffer.append(item)

    def sample(self, batch_size):
        """é«˜æ•ˆæ‰¹é‡é‡‡æ ·"""
        if self._prealloc_buffers:
            indices = np.random.choice(
                len(self) if not self._full else self.buffer.maxlen,
                batch_size,
                replace=False
            )
            return {
                'observations': torch.from_numpy(self._prealloc_buffers['observations'][indices]),
                'actions_chunk': torch.from_numpy(self._prealloc_buffers['actions_chunk'][indices]),
                'rewards_chunk': torch.from_numpy(self._prealloc_buffers['rewards_chunk'][indices]),
                'next_observations': torch.from_numpy(self._prealloc_buffers['next_observations'][indices]),
                'terminations': torch.from_numpy(self._prealloc_buffers['terminations'][indices])
            }
        else:
            batch = random.sample(self.buffer, batch_size)
            obs, actions, rewards, next_obs, dones = zip(*batch)
            return {
                'observations': torch.FloatTensor(np.array(obs)),
                'actions_chunk': torch.FloatTensor(np.array(actions)),
                'rewards_chunk': torch.FloatTensor(np.array(rewards)),
                'next_observations': torch.FloatTensor(np.array(next_obs)),
                'terminations': torch.FloatTensor(np.array(dones))
            }

    def __len__(self):
        return self._index if self._prealloc_buffers else len(self.buffer)


# ================== æ™ºèƒ½ä½“æ ¸å¿ƒ ==================
class QC_FQLAgent(nn.Module):
    """ä¼˜åŒ–åçš„QC-FQLæ™ºèƒ½ä½“ï¼ˆæ”¯æŒé«˜æ•ˆå¤šæ ·åŒ–æœ¬ç”Ÿæˆï¼‰"""

    def __init__(self, obs_dim, action_dim, horizon_length=5,
                 lr=3e-4, gamma=0.99, tau=0.005, alpha=100.0,
                 actor_type="best-of-n", num_samples=32,
                 flow_steps=10, device=None, action_space=None):
        super().__init__()
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.horizon_length = horizon_length
        self.gamma = gamma
        self.tau = tau
        # ä¿®å¤3: é™ä½alphaå‚æ•°ï¼Œé¿å…è’¸é¦æŸå¤±è¿‡å¤§
        self.alpha = min(alpha, 10.0)  # é™åˆ¶alphaæœ€å¤§å€¼
        self.actor_type = actor_type
        self.num_samples = num_samples
        self.flow_steps = flow_steps
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')

        # åŠ¨ä½œç©ºé—´å½’ä¸€åŒ–
        self.action_low = torch.tensor(action_space.low, device=self.device)
        self.action_high = torch.tensor(action_space.high, device=self.device)
        self.action_scale = (self.action_high - self.action_low) / 2
        self.action_bias = (self.action_high + self.action_low) / 2

        # ç½‘ç»œæ¶æ„
        self.flow_net = VectorizedFlowField(obs_dim, action_dim, horizon_length).to(self.device)
        self.critic = ParallelCritic(obs_dim, action_dim, horizon_length).to(self.device)
        self.target_critic = ParallelCritic(obs_dim, action_dim, horizon_length).to(self.device)
        self.target_critic.load_state_dict(self.critic.state_dict())

        # è’¸é¦ç­–ç•¥
        if actor_type == "distill":
            self.actor_net = VectorizedFlowField(obs_dim, action_dim, horizon_length).to(self.device)
            # ä¿®å¤4: ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡é¿å…è®­ç»ƒä¸ç¨³å®š
            self.actor_optim = optim.Adam(self.actor_net.parameters(), lr=lr * 0.5)

        # ä¿®å¤5: ä½¿ç”¨æ›´å°çš„åˆå§‹å­¦ä¹ ç‡ï¼Œç‰¹åˆ«æ˜¯å¯¹critic
        self.flow_optimizer = optim.Adam(self.flow_net.parameters(), lr=lr * 0.5)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr * 0.3)

        # è‡ªåŠ¨æ··åˆç²¾åº¦
        self.scaler = torch.amp.GradScaler(self.device) if self.device in ['cuda', 'cpu'] else None

        # è®­ç»ƒçŠ¶æ€
        self.step_count = 0

        # ä¿®å¤6: æ·»åŠ Qå€¼èŒƒå›´è·Ÿè¸ªï¼Œç”¨äºç¨³å®šæ€§ç›‘æ§
        self.q_running_mean = 0.0
        self.q_running_std = 1.0

    @torch.no_grad()
    def vectorized_flow_actions_from_noise(self, obs, noises):
        """ä»ç»™å®šå™ªå£°ç”ŸæˆåŠ¨ä½œï¼ˆç”¨äºdistill lossï¼‰"""
        batch_size = obs.shape[0]
        actions = noises.clone()

        # ä¿®å¤ï¼šæ­£ç¡®çš„æµåŒ¹é…ODEç§¯åˆ† (è®ºæ–‡Algorithm 3)
        dt = 1.0 / self.flow_steps
        for step in range(self.flow_steps):
            t_val = step * dt
            t = torch.full((actions.shape[0], 1), t_val, device=self.device)
            velocity = self.flow_net(obs, actions, t)
            actions = actions + velocity * dt  # ä¿®å¤ï¼šæ­£ç¡®çš„æ¬§æ‹‰ç§¯åˆ†

        # åŠ¨ä½œå½’ä¸€åŒ–
        actions = torch.tanh(actions)

        # ä¿®å¤ï¼šæ­£ç¡®çš„åŠ¨ä½œåå½’ä¸€åŒ– - å¤„ç†åŠ¨ä½œå—ç»´åº¦
        # actions shape: [batch_size, action_dim * horizon_length]
        # éœ€è¦é‡å¡‘ä¸º [batch_size, action_dim, horizon_length] æ¥åº”ç”¨å½’ä¸€åŒ–
        actions_reshaped = actions.view(batch_size, self.action_dim, self.horizon_length)

        # åº”ç”¨åŠ¨ä½œç©ºé—´çš„åå½’ä¸€åŒ–
        actions_denormalized = actions_reshaped * self.action_scale.unsqueeze(-1) + self.action_bias.unsqueeze(-1)

        # é‡æ–°å±•å¹³ä¸ºåŠ¨ä½œå—æ ¼å¼
        actions_denormalized = actions_denormalized.view(batch_size, -1)

        return actions_denormalized  # [batch, action_dim*h]

    @torch.no_grad()
    def vectorized_flow_actions(self, obs, num_samples=None):
        """å‘é‡åŒ–ç”Ÿæˆå¤šä¸ªåŠ¨ä½œæ ·æœ¬ï¼ˆä¿®å¤æµåŒ¹é…ç§¯åˆ†ï¼‰"""
        num_samples = num_samples or self.num_samples
        batch_size = obs.shape[0]

        # æ‰©å±•è§‚æµ‹
        obs_expanded = obs.repeat_interleave(num_samples, dim=0)

        # åˆå§‹å™ªå£°ï¼ˆæ‰¹é‡ç”Ÿæˆï¼‰
        actions = torch.randn(
            num_samples * batch_size,
            self.action_dim * self.horizon_length,
            device=self.device
        )

        # ä¿®å¤ï¼šæ­£ç¡®çš„æµåŒ¹é…ODEç§¯åˆ† (è®ºæ–‡Algorithm 3)
        dt = 1.0 / self.flow_steps
        for step in range(self.flow_steps):
            t_val = step * dt
            t = torch.full((actions.shape[0], 1), t_val, device=self.device)
            velocity = self.flow_net(obs_expanded, actions, t)
            actions = actions + velocity * dt  # ä¿®å¤ï¼šæ­£ç¡®çš„æ¬§æ‹‰ç§¯åˆ†

        # åŠ¨ä½œå½’ä¸€åŒ–
        actions = torch.tanh(actions)

        # ä¿®å¤ï¼šæ­£ç¡®çš„åŠ¨ä½œåå½’ä¸€åŒ– - å¤„ç†åŠ¨ä½œå—ç»´åº¦
        # actions shape: [num_samples * batch_size, action_dim * horizon_length]
        # éœ€è¦é‡å¡‘ä¸º [num_samples * batch_size, action_dim, horizon_length] æ¥åº”ç”¨å½’ä¸€åŒ–
        actions_reshaped = actions.view(num_samples * batch_size, self.action_dim, self.horizon_length)

        # åº”ç”¨åŠ¨ä½œç©ºé—´çš„åå½’ä¸€åŒ–
        actions_denormalized = actions_reshaped * self.action_scale.unsqueeze(-1) + self.action_bias.unsqueeze(-1)

        # é‡æ–°å±•å¹³ä¸ºåŠ¨ä½œå—æ ¼å¼
        actions_denormalized = actions_denormalized.view(num_samples * batch_size, -1)

        return actions_denormalized.view(batch_size, num_samples, -1)  # [batch, num_samples, action_dim*h]

    def sample_actions(self, obs, strategy=None, num_samples=None):
        """é«˜æ•ˆåŠ¨ä½œé‡‡æ ·ï¼ˆæ”¯æŒå¤šç§ç­–ç•¥ï¼‰"""
        strategy = strategy or self.actor_type
        num_samples = num_samples or self.num_samples
        obs = obs.to(self.device)

        if strategy == "best-of-n":
            # æ‰¹é‡ç”Ÿæˆå€™é€‰åŠ¨ä½œ
            candidate_actions = self.vectorized_flow_actions(obs, num_samples)  # [batch, num_samples, action_dim*h]

            # è¯„ä¼°Qå€¼
            batch_size = obs.shape[0]
            obs_expanded = obs.unsqueeze(1).repeat(1, num_samples, 1).view(-1, obs.shape[-1])
            q_values = self.critic(obs_expanded, candidate_actions.view(-1, candidate_actions.shape[-1]))
            q_values = q_values.view(batch_size, num_samples)

            # é€‰æ‹©æœ€ä½³åŠ¨ä½œ
            idx = q_values.argmax(dim=1)
            selected_actions = candidate_actions[torch.arange(batch_size), idx]
            # ç¡®ä¿è¾“å‡ºç»´åº¦æ­£ç¡®
            return selected_actions.view(batch_size, -1)

        elif strategy == "distill" and hasattr(self, 'actor_net') and self.actor_net is not None:
            # è’¸é¦ç­–ï¿½ï¿½ç›´æ¥ç”Ÿæˆ
            t_zero = torch.zeros(obs.shape[0], 1, device=self.device)
            # ä¿®å¤ï¼šåˆ›å»ºæ­£ç¡®ç»´åº¦çš„å™ªå£°å¼ é‡
            noise_input = torch.randn(obs.shape[0], self.action_dim * self.horizon_length, device=self.device)
            raw_actions = self.actor_net(obs, noise_input, t_zero)
            # ç¡®ä¿åŠ¨ä½œç»´åº¦æ­£ç¡®
            raw_actions = raw_actions.view(-1, self.action_dim, self.horizon_length)
            raw_actions = torch.tanh(raw_actions)
            raw_actions = raw_actions * self.action_scale.unsqueeze(-1) + self.action_bias.unsqueeze(-1)
            return raw_actions.view(obs.shape[0], self.action_dim * self.horizon_length)
        else:
            # é»˜è®¤æµé‡‡æ ·
            return self.vectorized_flow_actions(obs, num_samples=1).squeeze(1)

    def train_step(self, batch, offline_mode=False):
        """ä¼˜åŒ–çš„è®­ç»ƒæ­¥éª¤ï¼ˆä¿®å¤å…³é”®è®­ç»ƒé—®é¢˜ï¼‰"""
        # æ•°æ®è½¬ç§»åˆ°è®¾å¤‡
        obs = batch['observations'].to(self.device)
        actions_chunk = batch['actions_chunk'].to(self.device)
        rewards_chunk = batch['rewards_chunk'].to(self.device)
        next_obs = batch['next_observations'].to(self.device)
        dones = batch['terminations'].to(self.device).float()
        
        # ä¿®å¤ï¼šç¡®ä¿donesæ˜¯ä¸€ç»´å¼ é‡
        if dones.dim() > 1:
            dones = dones.squeeze(-1)
            
        B = obs.shape[0]

        # æ·»åŠ ç»´åº¦å®‰å…¨æ£€æŸ¥ï¼ˆè®ºæ–‡Section 4.3è¦æ±‚ï¼‰
        assert actions_chunk.shape == (B, self.action_dim * self.horizon_length), \
            f"åŠ¨ä½œå—ç»´åº¦é”™è¯¯ï¼šåº”ä¸º {(self.action_dim * self.horizon_length)}ï¼Œå®é™… {actions_chunk.shape[1]}"

        # ===== 1. BCæµåŒ¹é…è®­ç»ƒï¼ˆä¿®å¤åŠ¨ä½œå½’ä¸€åŒ–é—®é¢˜ï¼‰=====
        # ä¿®å¤ï¼šåŠ¨ä½œå½’ä¸€åŒ– - æ­£ç¡®å¤„ç†åŠ¨ä½œå—ç»´åº¦
        actions_reshaped = actions_chunk.view(B, self.action_dim, self.horizon_length)

        # å¯¹æ¯ä¸ªæ—¶é—´æ­¥çš„åŠ¨ä½œè¿›è¡Œå½’ä¸€åŒ–
        actions_normalized = (actions_reshaped - self.action_bias.unsqueeze(-1)) / self.action_scale.unsqueeze(-1)
        actions_normalized = torch.clamp(actions_normalized, -1.0, 1.0)

        # é‡æ–°å±•å¹³ä¸º [batch_size, action_dim * horizon_length]
        actions_normalized = actions_normalized.view(B, -1)

        # BC Flow Loss - æµåŒ¹é…æŸå¤±
        t = torch.rand(B, 1, device=self.device)
        noise = torch.randn_like(actions_normalized)
        x_t = (1 - t) * noise + t * actions_normalized
        target_velocity = actions_normalized - noise
        pred_velocity = self.flow_net(obs, x_t, t)

        # å¦‚æœä½¿ç”¨åŠ¨ä½œå—ï¼Œéœ€è¦æŒ‰æœ‰æ•ˆæ€§åŠ æƒ
        if hasattr(batch, 'valid') and 'valid' in batch:
            valid_mask = batch['valid'].to(self.device)
            # é‡å¡‘ä¸ºåŠ¨ä½œå—æ ¼å¼
            valid_reshaped = valid_mask.unsqueeze(-1).expand(-1, -1, self.action_dim).view(B, -1)
            bc_flow_loss = F.mse_loss(pred_velocity * valid_reshaped, target_velocity * valid_reshaped)
        else:
            bc_flow_loss = F.mse_loss(pred_velocity, target_velocity)

        # åˆå§‹åŒ–å…¶ä»–æŸå¤±
        distill_loss = torch.tensor(0.0, device=self.device)
        q_loss = torch.tensor(0.0, device=self.device)

        # ===== 2. Distill Lossï¼ˆä»…åœ¨distillæ¨¡å¼ä¸‹ï¼‰=====
        if self.actor_type == "distill" and hasattr(self, 'actor_net') and self.actor_net is not None:
            # ç”Ÿæˆç›®æ ‡åŠ¨ä½œï¼ˆæ¥è‡ªBCæµç½‘ç»œï¼‰
            with torch.no_grad():
                # ä½¿ç”¨ç›¸åŒçš„å™ªå£°ç”Ÿæˆç›®æ ‡åŠ¨ä½œ
                target_noises = torch.randn(B, self.action_dim * self.horizon_length, device=self.device)
                target_actions = self.vectorized_flow_actions_from_noise(obs, target_noises)
                # å½’ä¸€åŒ–ç›®æ ‡åŠ¨ä½œ
                target_actions_reshaped = target_actions.view(B, self.action_dim, self.horizon_length)
                target_actions_normalized = (target_actions_reshaped - self.action_bias.unsqueeze(-1)) / self.action_scale.unsqueeze(-1)
                target_actions_normalized = torch.clamp(target_actions_normalized, -1.0, 1.0)
                target_actions_normalized = target_actions_normalized.view(B, -1)

            # Actorç½‘ç»œç”ŸæˆåŠ¨ä½œï¼ˆä¸€æ­¥ç”Ÿæˆï¼‰
            actor_noises = torch.randn(B, self.action_dim * self.horizon_length, device=self.device)
            t_zero = torch.zeros(B, 1, device=self.device)
            actor_actions = self.actor_net(obs, actor_noises, t_zero)
            actor_actions = torch.clamp(actor_actions, -1.0, 1.0)

            # Distill Loss
            distill_loss = F.mse_loss(actor_actions, target_actions_normalized)

            # ===== 3. Q Lossï¼ˆActorçš„Qå€¼ä¼˜åŒ–ï¼‰=====
            # ä½¿ç”¨actorç”Ÿæˆçš„åŠ¨ä½œè®¡ç®—Qå€¼
            qs = self.critic(obs, actor_actions)
            if qs.dim() > 1:
                q = qs.mean(dim=0) if qs.shape[0] > 1 else qs.squeeze(0)
            else:
                q = qs
            q_loss = -q.mean()  # æœ€å¤§åŒ–Qå€¼

        # ===== 4. æ€»Actor Loss =====
        actor_loss = bc_flow_loss + self.alpha * distill_loss + q_loss

        # æ›´æ–°æµç½‘ç»œï¼ˆBCéƒ¨åˆ†ï¼‰
        self.flow_optimizer.zero_grad()
        if offline_mode:
            # ç¦»çº¿æ¨¡å¼åªè®­ç»ƒBCæµæŸå¤±
            bc_flow_loss.backward()
        else:
            # åœ¨çº¿æ¨¡å¼è®­ç»ƒå®Œæ•´çš„actor loss
            actor_loss.backward(retain_graph=True)
        torch.nn.utils.clip_grad_norm_(self.flow_net.parameters(), 1.0)
        self.flow_optimizer.step()

        # å¦‚æœæœ‰è’¸é¦ç½‘ç»œï¼Œå•ç‹¬æ›´æ–°
        if self.actor_type == "distill" and hasattr(self, 'actor_net') and not offline_mode:
            self.actor_optim.zero_grad()
            (distill_loss + q_loss).backward()
            torch.nn.utils.clip_grad_norm_(self.actor_net.parameters(), 1.0)
            self.actor_optim.step()

        # å¦‚æœæ˜¯ç¦»çº¿é¢„è®­ç»ƒæ¨¡å¼ï¼Œè·³è¿‡Criticè®­ç»ƒ
        if offline_mode:
            return {
                'flow_loss': bc_flow_loss.item(),
                'bc_flow_loss': bc_flow_loss.item(),
                'critic_loss': 0.0,
                'distill_loss': distill_loss.item(),
                'q_loss': q_loss.item(),
                'actor_loss': actor_loss.item(),
                'q_mean': 0.0,
                'q_min': 0.0,
                'q_max': 0.0,
                'target_q_mean': 0.0,
                'action_norm_mean': actions_normalized.abs().mean().item(),
            }

        # ===== 5. Criticè®­ç»ƒï¼ˆä¿®å¤ç›®æ ‡Qå€¼è®¡ç®—ï¼‰=====
        with torch.no_grad():
            # ç”Ÿæˆä¸‹ä¸€çŠ¶æ€çš„åŠ¨ä½œå—
            next_actions = self.sample_actions(next_obs)

            # æ­£ç¡®å¤„ç†ä¸‹ä¸€çŠ¶æ€åŠ¨ä½œçš„å½’ä¸€åŒ–
            next_actions_reshaped = next_actions.view(B, self.action_dim, self.horizon_length)
            next_actions_normalized = (next_actions_reshaped - self.action_bias.unsqueeze(-1)) / self.action_scale.unsqueeze(-1)
            next_actions_normalized = torch.clamp(next_actions_normalized, -1.0, 1.0)
            next_actions_normalized = next_actions_normalized.view(B, self.action_dim * self.horizon_length)

            # è®¡ç®—æœªæ¥ä»·å€¼
            next_q = self.target_critic(next_obs, next_actions_normalized)
            next_q = next_q.squeeze(-1) if next_q.dim() > 1 else next_q

            # ä¿®å¤2: æ­£ç¡®çš„hæ­¥ä»·å€¼å¤‡ä»½æœºåˆ¶ï¼ˆè®ºæ–‡Eq.7ï¼‰
            # åˆ›å»ºæ—¶é—´æ­¥ç´¢å¼• [0, 1, ..., h-1]
            time_steps = torch.arange(self.horizon_length, device=self.device, dtype=torch.float32)
            # è®¡ç®—æŠ˜æ‰£å› å­ [Î³^0, Î³^1, ..., Î³^(h-1)]
            discounts = torch.pow(self.gamma, time_steps)  # [horizon_length]

            # å¤„ç†æå‰ç»ˆæ­¢ï¼šåˆ›å»ºæœ‰æ•ˆæ€§æ©ç 
            # dones shape: [B], éœ€è¦æ‰©å±•ä¸º [B, horizon_length]
            done_mask = dones.unsqueeze(1).expand(-1, self.horizon_length)  # [B, horizon_length]

            # å¦‚æœepisodeåœ¨æŸæ­¥ç»ˆæ­¢ï¼Œåç»­å¥–åŠ±åº”ç½®é›¶
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå‡è®¾ç»ˆæ­¢å‘ç”Ÿåœ¨chunkçš„æœ€å
            valid_rewards = rewards_chunk * (1 - done_mask)  # [B, horizon_length]

            # éªŒè¯ç»´åº¦
            assert valid_rewards.shape == (B, self.horizon_length), \
                f"rewards shape mismatch: {valid_rewards.shape} vs ({B}, {self.horizon_length})"
            assert discounts.shape == (self.horizon_length,), \
                f"discounts shape mismatch: {discounts.shape} vs ({self.horizon_length},)"

            # è®¡ç®—hæ­¥æŠ˜æ‰£å¥–åŠ±ç´¯ç§¯ (è®ºæ–‡Eq.7çš„ç´¯ç§¯é¡¹)
            discounted_rewards = torch.sum(valid_rewards * discounts.unsqueeze(0), dim=1)  # [B]

            # è®¡ç®—ç›®æ ‡Qå€¼ï¼šhæ­¥å¥–åŠ± + Î³^h * æœªæ¥ä»·å€¼
            # å¦‚æœepisodeå·²ç»ˆæ­¢ï¼Œæœªæ¥ä»·å€¼ä¸º0
            future_value_mask = (1 - dones)  # [B] - å¦‚æœç»ˆæ­¢åˆ™æ©ç›–æœªæ¥ä»·å€¼
            target_q = discounted_rewards + (self.gamma ** self.horizon_length) * future_value_mask * next_q

            # æœ€ç»ˆç»´åº¦æ£€æŸ¥
            assert target_q.shape == (B,), f"target_q shape: {target_q.shape}, expected: ({B},)"

        # å½“å‰Qä¼°è®¡
        current_q = self.critic(obs, actions_normalized)
        current_q = current_q.squeeze(-1) if current_q.dim() > 1 else current_q

        # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ç»´åº¦åŒ¹é…
        assert current_q.shape == target_q.shape == (B,), \
            f"Shape mismatch - current_q: {current_q.shape}, target_q: {target_q.shape}, expected: ({B},)"

        critic_loss = F.mse_loss(current_q, target_q)

        # æ›´æ–°Critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)
        self.critic_optimizer.step()

        # ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°
        self.soft_update_target()
        self.step_count += 1

        # è¿”å›å¢å¼ºçš„ç›‘æ§æŒ‡æ ‡
        return {
            'flow_loss': bc_flow_loss.item(),
            'bc_flow_loss': bc_flow_loss.item(),
            'critic_loss': critic_loss.item(),
            'distill_loss': distill_loss.item(),
            'q_loss': q_loss.item(),
            'actor_loss': actor_loss.item(),
            'q_mean': current_q.mean().item(),
            'q_min': current_q.min().item(),
            'q_max': current_q.max().item(),
            'target_q_mean': target_q.mean().item(),
            'action_norm_mean': actions_normalized.abs().mean().item(),
            'discounted_rewards_mean': discounted_rewards.mean().item(),  # æ–°å¢ç›‘æ§
            'future_value_mean': (future_value_mask * next_q).mean().item(),  # æ–°å¢ç›‘æ§
        }

    def soft_update_target(self):
        """ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°"""
        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

    @torch.no_grad()
    def get_action(self, obs, execute_length=None):
        """å®æ—¶åŠ¨ä½œç”Ÿæˆï¼Œæ”¯æŒå•ä¸ªæˆ–æ‰¹é‡è§‚æµ‹è¾“å…¥"""
        # ç¡®ä¿è¾“å…¥æ˜¯å¼ é‡å¹¶æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        if not isinstance(obs, torch.Tensor):
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
        else:
            obs_tensor = obs.unsqueeze(0) if obs.dim() == 1 else obs
            
        # ç”ŸæˆåŠ¨ä½œå—
        action_chunk = self.sample_actions(obs_tensor).cpu().numpy().flatten()
        
        # ç¡®ä¿åŠ¨ä½œç»´åº¦æ­£ç¡®
        assert len(action_chunk) == self.action_dim * self.horizon_length, \
            f"åŠ¨ä½œå—ç»´åº¦é”™è¯¯: {len(action_chunk)} != {self.action_dim} * {self.horizon_length}"
        
        # è¿”å›æŒ‡å®šé•¿åº¦çš„åŠ¨ä½œ
        if execute_length is not None:
            # ç¡®ä¿æ‰§è¡Œé•¿åº¦ä¸ä¼šè¶…å‡ºåŠ¨ä½œå—é•¿åº¦
            execute_length = min(execute_length, self.horizon_length)
            return action_chunk[:execute_length * self.action_dim]
        return action_chunk


# ================== è®­ç»ƒå’Œè¯„ä¼°æ¡†æ¶ ==================
def load_minari_dataset(dataset_name, num_episodes=None):
    """åŠ è½½Minariæ•°æ®é›†å¹¶æå–è½¨è¿¹"""
    dataset = minari.load_dataset(dataset_name,download= True)
    episodes = []

    # æŒ‰éœ€åŠ è½½éƒ¨åˆ†æ•°æ®é›†
    for i in range(min(num_episodes or len(dataset), len(dataset))):
        episode = dataset[i]
        episodes.append({
            'observations': episode.observations,
            'actions': episode.actions,
            'rewards': episode.rewards,
            'terminations': episode.terminations
        })

    return episodes


def fill_buffer_from_episodes(buffer, episodes):
    """ç”¨æ•°æ®é›†å¡«å……ç¼“å†²åŒº"""
    print(f"Filling buffer with {len(episodes)} episodes...")
    start_time = time.time()

    for ep in episodes:
        buffer.add_trajectory(
            ep['observations'],
            ep['actions'],
            ep['rewards'],
            ep['terminations']
        )

    print(f"Buffer filled with {len(buffer)} chunks in {time.time() - start_time:.1f}s")


def run_episode(agent, env, horizon_length, max_steps=1000, render=False, deterministic=False):
    """æ‰§è¡Œä¸€ä¸ªepisodeï¼ˆä¿®å¤ï¼šåŠ¨ä½œå—åŸå­æ€§æ‰§è¡Œï¼‰"""
    obs, _ = env.reset()
    
    # è§‚æµ‹ç»´åº¦é€‚é…
    if len(obs) != agent.obs_dim:
        if len(obs) > agent.obs_dim:
            obs = obs[:agent.obs_dim]
        else:
            padded_obs = np.zeros(agent.obs_dim)
            padded_obs[:len(obs)] = obs
            obs = padded_obs
    
    total_reward = 0
    step_count = 0

    while step_count < max_steps:
        # æ ¸å¿ƒä¿®å¤ï¼šåŸå­æ€§æ‰§è¡ŒåŠ¨ä½œå—ï¼ˆè®ºæ–‡Section 4.1è¦æ±‚ï¼‰
        action_chunk = agent.get_action(obs)  # ç”Ÿæˆå®Œæ•´åŠ¨ä½œå—

        # å°†åŠ¨ä½œå—é‡å¡‘ä¸º[horizon_length, action_dim]ä»¥æŒ‰æ­¥æ‰§è¡Œ
        actions_to_execute = action_chunk.reshape(horizon_length, agent.action_dim)

        # åŸå­æ€§æ‰§è¡Œæ•´ä¸ªåŠ¨ä½œå—ï¼ˆè®ºæ–‡Fig.1å·¦ä¾§è¦æ±‚ï¼‰
        chunk_reward = 0
        chunk_obs = obs  # è®°å½•åŠ¨ä½œå—å¼€å§‹æ—¶çš„è§‚æµ‹

        for step_in_chunk in range(horizon_length):
            if step_count >= max_steps:
                break

            action = actions_to_execute[step_in_chunk]

            # ç¯å¢ƒäº¤äº’
            next_obs, reward, terminated, truncated, _ = env.step(action)

            # è§‚æµ‹ç»´åº¦é€‚é…
            if len(next_obs) != agent.obs_dim:
                if len(next_obs) > agent.obs_dim:
                    next_obs = next_obs[:agent.obs_dim]
                else:
                    padded_obs = np.zeros(agent.obs_dim)
                    padded_obs[:len(next_obs)] = next_obs
                    next_obs = padded_obs

            if render:
                env.render()
                time.sleep(0.01)

            chunk_reward += reward
            step_count += 1
            obs = next_obs  # æ›´æ–°è§‚æµ‹ç”¨äºä¸‹ä¸€ä¸ªåŠ¨ä½œå—

            if terminated or truncated:
                break

        total_reward += chunk_reward

        if terminated or truncated:
            break

    return total_reward, step_count


def evaluate_agent(agent, env, horizon_length, num_episodes=5, render=False, deterministic=False):
    """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½"""
    total_rewards = []
    total_steps = []

    for ep in range(num_episodes):
        try:
            reward, steps = run_episode(
                agent, env, horizon_length,
                render=render, deterministic=deterministic
            )
            total_rewards.append(reward)
            total_steps.append(steps)
            print(f"Episode {ep + 1}: Steps={steps}, Reward={reward:.1f}")
        except Exception as e:
            print(f"Episode {ep + 1}: Error during evaluation: {str(e)}")
            continue

    if len(total_rewards) > 0:
        avg_reward = np.mean(total_rewards)
        avg_steps = np.mean(total_steps)
        print(f"Evaluation over {len(total_rewards)} episodes: Avg Steps={avg_steps:.1f}, Avg Reward={avg_reward:.2f}")
        return avg_reward
    else:
        print("No successful episodes in evaluation")
        return float('-inf')


# ================== ä¸»è®­ç»ƒæµç¨‹ ==================
def train_qc_fql_agent():
    """QC-FQLè®­ç»ƒä¸»æµç¨‹"""
    parser = argparse.ArgumentParser(description="é«˜æ•ˆQC-FQLå®ç°")
    parser.add_argument('--env_id', type=str, default='Ant-v5', help='Gymç¯å¢ƒID')
    parser.add_argument('--dataset_name', type=str, default='mujoco/ant/expert-v0', help='Minariæ•°æ®é›†')
    parser.add_argument('--horizon', type=int, default=5, help='åŠ¨ä½œå—é•¿åº¦')
    parser.add_argument('--num_samples', type=int, default=32, help='Best-of-Né‡‡æ ·æ•°')
    parser.add_argument('--flow_steps', type=int, default=10, help='æµåŒ¹é…æ­¥æ•°')
    parser.add_argument('--actor_type', type=str, default='distill', choices=['best-of-n', 'distill'])
    parser.add_argument('--batch_size', type=int, default=256, help='è®­ç»ƒæ‰¹å¤§å°')
    parser.add_argument('--num_updates', type=int, default=10000, help='è®­ç»ƒæ›´æ–°æ¬¡æ•°')
    parser.add_argument('--eval_freq', type=int, default=2000, help='è¯„ä¼°é¢‘ç‡')
    parser.add_argument('--num_eval_episodes', type=int, default=3, help='è¯„ä¼°episodeæ•°')
    parser.add_argument('--num_init_episodes', type=int, default=100, help='åˆå§‹åŒ–æ•°æ®é›†episodeæ•°')
    parser.add_argument('--render_eval', action='store_true',default=True, help='è¯„ä¼°æ—¶æ¸²æŸ“')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--device', type=str, default='auto', help='è®¡ç®—è®¾å¤‡')
    args = parser.parse_args()

    # åŠ è½½æ•°æ®é›†
    print(f"åŠ è½½æ•°æ®é›†: {args.dataset_name}")
    episodes = load_minari_dataset(args.dataset_name, args.num_init_episodes)
    
    # è·å–æ•°æ®é›†ä¸­å®é™…çš„è§‚æµ‹ç»´åº¦å’ŒåŠ¨ä½œç»´åº¦
    sample_ep = episodes[0]
    dataset_obs_shape = sample_ep['observations'].shape
    dataset_obs_dim = dataset_obs_shape[1] if len(dataset_obs_shape) > 1 else dataset_obs_shape[0]
    
    dataset_action_shape = sample_ep['actions'].shape
    dataset_action_dim = dataset_action_shape[1] if len(dataset_action_shape) > 1 else dataset_action_shape[0]
    
    print(f"æ•°æ®é›†ä¸­è§‚æµ‹ç»´åº¦: {dataset_obs_dim} (shape: {dataset_obs_shape})")
    print(f"æ•°æ®é›†ä¸­åŠ¨ä½œç»´åº¦: {dataset_action_dim} (shape: {dataset_action_shape})")

    # åˆå§‹åŒ–ç¯å¢ƒ
    set_seed(args.seed)
    device = args.device if args.device != 'auto' else ('cuda' if torch.cuda.is_available() else 'cpu')

    # ä»æ•°æ®é›†æ¢å¤ç¯å¢ƒè€Œä¸æ˜¯ä½¿ç”¨gym.make
    print(f"ä»æ•°æ®é›†æ¢å¤ç¯å¢ƒ: {args.dataset_name}")
    dataset = minari.load_dataset(args.dataset_name)

    # åˆ›å»ºè®­ç»ƒç¯å¢ƒï¼ˆä¸å¸¦æ¸²æŸ“ï¼‰
    env = dataset.recover_environment()

    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒï¼ˆæ˜ç¡®å…³é—­æ¸²æŸ“æ¨¡å¼ï¼‰
    if args.render_eval:
        try:
            eval_env = dataset.recover_environment(eval_env=True, render_mode='human')
            print("è¯„ä¼°ç¯å¢ƒå·²è®¾ç½®ä¸ºæ¸²æŸ“æ¨¡å¼")
        except Exception as e:
            print(f"æ¸²æŸ“æ¨¡å¼è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨æ— æ¸²æŸ“æ¨¡å¼: {e}")
            eval_env = dataset.recover_environment(eval_env=True, render_mode=None)
    else:
        eval_env = dataset.recover_environment(eval_env=True, render_mode=None)
        print("è¯„ä¼°ç¯å¢ƒå·²è®¾ç½®ä¸ºæ— æ¸²æŸ“æ¨¡å¼")

    env_obs_dim = env.observation_space.shape[0]
    env_action_dim = env.action_space.shape[0]
    print(f"ç¯å¢ƒè§‚æµ‹ç»´åº¦: {env_obs_dim}, åŠ¨ä½œç»´åº¦: {env_action_dim}")
    
    # ============ ç®€ï¿½ï¿½ï¿½çš„åŠ¨ä½œç©ºé—´å¤„ç† ============
    print("\n=== åŠ¨ä½œç©ºé—´é€‚é…æ£€ï¿½ï¿½ï¿½ ===")

    # ç›´æ¥ï¿½ï¿½gymåˆ›å»ºç¯å¢ƒè·å–æ ‡å‡†ï¿½ï¿½ï¿½ï¿½ä½œç©ºé—´ï¼ˆå¦‚æœå¯èƒ½ï¼‰
    try:
        gym_env = gym.make(args.env_id)
        gym_action_space = gym_env.action_space
        print(f"Gymæ ‡å‡†ç¯å¢ƒåŠ¨ä½œç©ºé—´: {gym_action_space}")
        gym_env.close()
    except Exception as e:
        print(f"æ— æ³•åˆ›å»ºæ ‡å‡†gymç¯å¢ƒ: {e}")
        gym_action_space = None

    # ä»æ•°æ®é›†è·å–å®é™…åŠ¨ä½œä¿¡æ¯
    dataset_action_low = np.min([ep['actions'].min() for ep in episodes])
    dataset_action_high = np.max([ep['actions'].max() for ep in episodes])
    print(f"æ•°æ®é›†åŠ¨ä½œèŒƒå›´: [{dataset_action_low:.3f}, {dataset_action_high:.3f}]")
    print(f"æ•°æ®é›†åŠ¨ä½œç»´åº¦: {dataset_action_dim}")
    print(f"ç¯å¢ƒè§‚æµ‹ç»´åº¦: {env_obs_dim}, åŠ¨ä½œç»´åº¦: {env_action_dim}")

    # æ™ºèƒ½é€‰æ‹©åŠ¨ä½œç©ºé—´é…ç½®
    if gym_action_space is not None and gym_action_space.shape[0] == dataset_action_dim:
        # å¦‚æœgymç¯å¢ƒå­˜åœ¨ä¸”ç»´åº¦åŒ¹é…ï¼Œä½¿ç”¨gymçš„åŠ¨ä½œç©ºé—´
        print("âœ… ä½¿ç”¨Gymæ ‡å‡†åŠ¨ä½œç©ºé—´")
        action_space_for_training = gym_action_space
    else:
        # å¦åˆ™åŸºäºæ•°æ®é›†åˆ›å»ºåŠ¨ä½œç©ºé—´
        print("âš ï¸  ä½¿ç”¨æ•°æ®é›†æ¨æ–­çš„åŠ¨ä½œç©ºé—´")
        action_space_for_training = gym.spaces.Box(
            low=dataset_action_low,
            high=dataset_action_high,
            shape=(dataset_action_dim,),
            dtype=np.float32
        )

    print(f"è®­ç»ƒç”¨åŠ¨ä½œç©ºé—´: {action_space_for_training}")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦åŠ¨ä½œè½¬æ¢
    need_action_conversion = (env_action_dim != dataset_action_dim)
    if need_action_conversion:
        print(f"ğŸ”„ è¯„ä¼°æ—¶éœ€è¦åŠ¨ä½œï¿½ï¿½ï¿½åº¦è½¬æ¢: {dataset_action_dim} -> {env_action_dim}")
    else:
        print("âœ… åŠ¨ä½œç»´åº¦åŒ¹é…ï¿½ï¿½ï¿½æ— éœ€è½¬æ¢")

    # ä½¿ç”¨æ•°ï¿½ï¿½ï¿½é›†ç»´åº¦è¿›è¡Œï¿½ï¿½ï¿½ï¿½ï¿½ç»ƒ
    obs_dim = dataset_obs_dim
    action_dim = dataset_action_dim
    

    # åˆå§‹åŒ–ç¼“å†²åŒº
    print(f"åˆå§‹åŒ–ç¼“å†²åŒº (horizon={args.horizon}, capacity=1000000)")
    buffer = VectorizedReplayBuffer(horizon_length=args.horizon)

    # é¢„åˆ†é…å†…å­˜
    buffer._preallocate_buffers({
        'observations': (obs_dim,),
        'actions_chunk': (action_dim * args.horizon,),
        'rewards_chunk': (args.horizon,),
        'next_observations': (obs_dim,),
        'terminations': (1,)
    })

    # å¡«å……ç¼“å†²åŒº
    fill_buffer_from_episodes(buffer, episodes)

    # åˆå§‹åŒ–æ™ºèƒ½ä½“
    print(f"åˆå§‹åŒ–QC-FQLæ™ºèƒ½ä½“ (actor_type={args.actor_type})")
    agent = QC_FQLAgent(
        obs_dim=obs_dim,
        action_dim=action_dim,
        horizon_length=args.horizon,
        flow_steps=args.flow_steps,
        num_samples=args.num_samples,
        actor_type=args.actor_type,
        action_space=action_space_for_training,  # ä½¿ç”¨æ™ºèƒ½é€‰æ‹©çš„åŠ¨ä½œç©ºé—´
        device=device
    )

    # ä¿®å¤4: æ·»åŠ ç¦»çº¿é¢„è®­ç»ƒé˜¶æ®µï¼ˆè®ºæ–‡Section 5.2ï¼‰
    offline_pretrain_steps = 10000  # æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´
    print(f"\n===== ç¦»çº¿é¢„è®­ç»ƒé˜¶æ®µ =====")
    print(f"é¢„è®­ç»ƒæµç½‘ç»œ {offline_pretrain_steps} æ­¥...")

    pretrain_progress = tqdm.tqdm(range(offline_pretrain_steps), desc="ç¦»çº¿é¢„è®­ç»ƒ")
    pretrain_losses = []

    for pretrain_step in pretrain_progress:
        if len(buffer) >= args.batch_size:
            batch = buffer.sample(args.batch_size)
            metrics = agent.train_step(batch, offline_mode=True)  # ä»…è®­ç»ƒæµç½‘ç»œ
            pretrain_losses.append(metrics['flow_loss'])

            # æ›´æ–°è¿›åº¦æ¡
            pretrain_progress.set_postfix({
                'FlowLoss': f"{metrics['flow_loss']:.4f}",
                'ActionNorm': f"{metrics['action_norm_mean']:.3f}",
            })

            # å®šæœŸæ£€æŸ¥é¢„è®­ç»ƒæ”¶æ•›
            if pretrain_step % 1000 == 0 and pretrain_step > 0:
                recent_losses = pretrain_losses[-500:] if len(pretrain_losses) >= 500 else pretrain_losses
                avg_loss = np.mean(recent_losses)
                loss_std = np.std(recent_losses)
                print(f"\n  é¢„è®­ç»ƒè¿›åº¦ {pretrain_step}/{offline_pretrain_steps}: æµæŸå¤±å‡å€¼={avg_loss:.6f}, æ ‡å‡†å·®={loss_std:.6f}")

                # æ—©åœæ£€æŸ¥ï¼šå¦‚æœæŸå¤±å·²ç»å¾ˆç¨³å®šï¼Œå¯ä»¥æå‰ç»“æŸ
                if len(recent_losses) >= 500 and loss_std < 0.001:
                    print(f"  âœ… æµæŸå¤±å·²æ”¶æ•›ï¼Œæå‰ç»“æŸé¢„è®­ç»ƒ")
                    break

    print(f"ç¦»çº¿é¢„è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæµæŸå¤±: {pretrain_losses[-1]:.6f}")

    # é¢„è®­ç»ƒåçš„éªŒè¯ï¼šæ£€æŸ¥æµç½‘ç»œæ˜¯å¦å­¦ä¼šæ¨¡ä»¿
    print("\n[é¢„è®­ç»ƒéªŒè¯] æµ‹è¯•æµç½‘ç»œæ¨¡ä»¿èƒ½åŠ›...")
    agent.eval()
    with torch.no_grad():
        # éšæœºé‡‡æ ·ä¸€äº›è§‚æµ‹ï¼Œæµ‹è¯•åŠ¨ä½œç”Ÿæˆè´¨é‡
        test_batch = buffer.sample(min(32, len(buffer)))
        test_obs = test_batch['observations'][:5].to(device)  # å–5ä¸ªæ ·æœ¬

        # ç”ŸæˆåŠ¨ä½œå¹¶æ£€æŸ¥åˆç†æ€§
        generated_actions = agent.sample_actions(test_obs)
        print(f"  ç”ŸæˆåŠ¨ä½œèŒƒå›´: [{generated_actions.min().item():.3f}, {generated_actions.max().item():.3f}]")
        print(f"  ç”ŸæˆåŠ¨ä½œå‡å€¼: {generated_actions.mean().item():.3f}")
        print(f"  åŠ¨ä½œå—ç»´åº¦: {generated_actions.shape}")
    agent.train()

    # è®­ç»ƒå¾ªç¯
    print(f"\n===== åœ¨çº¿å¼ºåŒ–å­¦ä¹ é˜¶æ®µ =====")
    print(f"å¼€å§‹è®­ç»ƒ: {args.num_updates}æ¬¡æ›´æ–° (batch_size={args.batch_size})")
    best_reward = -np.inf
    progress = tqdm.tqdm(range(args.num_updates), desc="è®­ç»ƒ")
    metrics_history = []

    # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
    flow_scheduler = optim.lr_scheduler.CosineAnnealingLR(agent.flow_optimizer, T_max=args.num_updates)
    critic_scheduler = optim.lr_scheduler.CosineAnnealingLR(agent.critic_optimizer, T_max=args.num_updates)
    if hasattr(agent, 'actor_optim'):
        actor_scheduler = optim.lr_scheduler.CosineAnnealingLR(agent.actor_optim, T_max=args.num_updates)

    for step in progress:
        # é‡‡æ ·æ‰¹æ¬¡å¹¶è®­ç»ƒ
        if len(buffer) >= args.batch_size:
            batch = buffer.sample(args.batch_size)
            metrics = agent.train_step(batch)
            metrics_history.append(metrics)

            # æ›´æ–°å­¦ä¹ ç‡
            flow_scheduler.step()
            critic_scheduler.step()
            if hasattr(agent, 'actor_optim'):
                actor_scheduler.step()

            # æ›´æ–°è¿›åº¦æ¡
            progress.set_postfix({
                'FlowLoss': f"{metrics['flow_loss']:.4f}",
                'CriticLoss': f"{metrics['critic_loss']:.4f}",
                'QMean': f"{metrics['q_mean']:.2f}",
                'QRange': f"[{metrics['q_min']:.1f},{metrics['q_max']:.1f}]",
                'ActNorm': f"{metrics['action_norm_mean']:.3f}",
                'FlowLR': f"{flow_scheduler.get_last_lr()[0]:.1e}"
            })

            # å¢ï¿½ï¿½æ”¶æ•›æ€§è¯Šæ–­
            if step % 500 == 0 and step > 0:
                print(f"\n[æ”¶æ•›è¯Šæ–­] Step {step}: è¯¦ç»†åˆ†æ")
                recent_metrics = metrics_history[-100:] if len(metrics_history) >= 100 else metrics_history

                # Flow Lossè¶‹åŠ¿åˆ†æ
                recent_flow_loss = [m['flow_loss'] for m in recent_metrics]
                recent_critic_loss = [m['critic_loss'] for m in recent_metrics]
                recent_q_mean = [m['q_mean'] for m in recent_metrics]

                print(f"  Flow Loss: å½“å‰={metrics['flow_loss']:.6f}, å¹³å‡={np.mean(recent_flow_loss):.6f}, è¶‹åŠ¿={'ä¸‹é™' if len(recent_flow_loss) > 10 and recent_flow_loss[-5:] < recent_flow_loss[:5] else 'ç¨³å®š/ä¸Šå‡'}")
                print(f"  Critic Loss: å½“å‰={metrics['critic_loss']:.6f}, å¹³å‡={np.mean(recent_critic_loss):.6f}")
                print(f"  Qå€¼èŒƒå›´: [{metrics['q_min']:.2f}, {metrics['q_max']:.2f}], ï¿½ï¿½æ ‡Qå‡å€¼={metrics['target_q_mean']:.2f}")
                print(f"  åŠ¨ä½œå½’ä¸€åŒ–å‡ï¿½ï¿½ï¿½: {metrics['action_norm_mean']:.4f} (åº”æ¥è¿‘0.5)")

                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨çˆ†ç‚¸æˆ–æ¶ˆå¤±
                if metrics['q_mean'] > 1e4 or metrics['q_mean'] < -1e4:
                    print(f"  âš ï¸  Qå€¼å¯ï¿½ï¿½ï¿½çˆ†ç‚¸! ï¿½ï¿½ï¿½å‰Qå‡å€¼: {metrics['q_mean']:.2f}")
                    # ç´§æ€¥ä¿®å¤ï¼šé™ä½å­¦ä¹ ç‡
                    for param_group in agent.flow_optimizer.param_groups:
                        param_group['lr'] *= 0.5
                    for param_group in agent.critic_optimizer.param_groups:
                        param_group['lr'] *= 0.5
                    print(f"  ğŸ”§ å·²ç´§æ€¥é™ä½å­¦ä¹ ç‡åˆ° {agent.flow_optimizer.param_groups[0]['lr']:.2e}")

                # æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
                flow_grad_norm = 0
                critic_grad_norm = 0
                for p in agent.flow_net.parameters():
                    if p.grad is not None:
                        flow_grad_norm += p.grad.data.norm(2).item() ** 2
                for p in agent.critic.parameters():
                    if p.grad is not None:
                        critic_grad_norm += p.grad.data.norm(2).item() ** 2

                flow_grad_norm = flow_grad_norm ** 0.5
                critic_grad_norm = critic_grad_norm ** 0.5
                print(f"  æ¢¯åº¦èŒƒæ•°: Flow={flow_grad_norm:.4f}, Critic={critic_grad_norm:.4f}")

                # åŠ¨ä½œç”Ÿæˆæµ‹ï¿½ï¿½ï¿½
                with torch.no_grad():
                    test_obs = torch.randn(1, obs_dim, device=device)
                    test_actions = agent.sample_actions(test_obs)
                    print(f"  æµ‹è¯•åŠ¨ä½œ: èŒƒå›´=[{test_actions.min().item():.3f}, {test_actions.max().item():.3f}], å‡å€¼={test_actions.mean().item():.3f}")

                # æ”¶æ•›æ€§å»ºè®®
                if len(recent_flow_loss) >= 50:
                    flow_stability = np.std(recent_flow_loss[-25:]) / (np.mean(recent_flow_loss[-25:]) + 1e-8)
                    if flow_stability < 0.1:
                        print(f"  âœ… Flow Losså·²è¶‹äºç¨³å®š (å˜å¼‚ç³»æ•°: {flow_stability:.3f})")
                    elif flow_stability > 0.5:
                        print(f"  âš ï¸  Flow Losséœ‡è¡è¾ƒå¤§ (å˜å¼‚ç³»æ•°: {flow_stability:.3f})")

                print(f"  ï¿½ï¿½ä¹ ç‡: Flow={flow_scheduler.get_last_lr()[0]:.2e}, Critic={critic_scheduler.get_last_lr()[0]:.2e}")

        # å®šæœŸè¯„ä¼°
        if step % args.eval_freq == 0 and step > 0:
            print(f"\nè¯„ä¼° @ step {step}/{args.num_updates}")
            avg_reward = evaluate_agent(
                agent, eval_env, args.horizon,  # ä½¿ç”¨è¯„ä¼°ç¯å¢ƒ
                num_episodes=args.num_eval_episodes,
                render=args.render_eval
            )

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_reward > best_reward:
                best_reward = avg_reward
                model_path = f"qc_fql_{args.dataset_name.replace('/', '_')}_best.pt"
                torch.save(agent.state_dict(), model_path)
                print(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° {model_path} (å¥–åŠ±={best_reward:.2f})")

        # æ—©æœŸè¯„ä¼°æ£€æŸ¥ - åœ¨è®­ç»ƒåˆæœŸå°±åšä¸€æ¬¡è¯„ä¼°çœ‹çœ‹baseline
        if step == 100:
            print(f"\n[æ—©æœŸè¯„ä¼°] @ step {step} - æ£€æŸ¥åˆå§‹æ€§èƒ½")
            early_reward = evaluate_agent(
                agent, eval_env, args.horizon,
                num_episodes=1,
                render=False
            )
            print(f"æ—©æœŸè¯„ä¼°å¥–åŠ±: {early_reward:.2f}")

            # çº¯ï¿½ï¿½ï¿½ä»¿å­¦ä¹ åŸºçº¿æµ‹è¯•
            print("\n[çº¯æ¨¡ä»¿æµ‹è¯•] æµ‹è¯•æµç½‘ç»œæ˜¯å¦å­¦ä¼šäº†åŸºæœ¬çš„æ¨¡ä»¿...")
            agent.eval()
            try:
                imitation_reward = evaluate_agent(
                    agent, eval_env, args.horizon,
                    num_episodes=1,
                    render=False,
                    deterministic=True
                )
                print(f"çº¯æ¨¡ä»¿å¥–åŠ±: {imitation_reward:.2f}")
            except Exception as e:
                print(f"çº¯æ¨¡ä»¿æµ‹è¯•å¤±ï¿½ï¿½: {e}")
            agent.train()

    # æœ€ç»ˆè¯„ä¼°å’Œä¿å­˜
    print("\n===== æœ€ç»ˆè¯„ä¼° =====")
    final_reward = evaluate_agent(
        agent, eval_env, args.horizon,  # ä½¿ç”¨è¯„ä¼°ç¯å¢ƒ
        num_episodes=10,
        render=args.render_eval
    )
    print(f"æœ€ç»ˆå¥–åŠ±: {final_reward:.2f} | æœ€ä½³å¥–åŠ±: {best_reward:.2f}")

    final_model_path = f"qc_fql_{args.dataset_name.replace('/', '_')}_final.pt"
    torch.save(agent.state_dict(), final_model_path)
    print(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {final_model_path}")

    env.close()
    eval_env.close()


if __name__ == "__main__":
    train_qc_fql_agent()